# Source: "Think Artificial Intelligence" by Jerry Cuomo, 2024
# Purpose: To illustrate fine-tuning techniques for pre-trained models on custom datasets.
# Copyright Â© 2024 Jerry Cuomo. All rights reserved.
#
# This code was autogenerated by GPT-4, following specific guidance:
# Prompt: Demonstrate fine-tuning the BERT model on a custom text dataset for the task of Masked Language Modeling (MLM), showing the full process from data preparation to model training.
#
# About: This script demonstrates the fine-tuning of the BERT model for MLM using a custom text dataset. It includes steps for loading the tokenizer and model, preparing the dataset by encoding text into suitable input formats, creating a PyTorch dataset for MLM, setting up training parameters, and initiating the training process with the Trainer class. The example showcases how to handle text data, split it into manageable chunks, mask tokens for MLM, and fine-tune a pre-trained BERT model to better understand and generate language based on the specific dataset.
#
# Setup: Ensure Python, PyTorch, and the transformers library are installed. The script requires a custom text dataset located at 'datasets/BK900687791.txt'. Install necessary libraries using 'pip install transformers torch'.

from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments
from transformers import TextDataset, DataCollatorForLanguageModeling

def fine_tune_bert(book_path, model_name='bert-base-uncased'):
    # Load the tokenizer and model
    tokenizer = BertTokenizer.from_pretrained(model_name)
    model = BertForMaskedLM.from_pretrained(model_name)

    # Prepare dataset
    def encode_book(file_path):
        examples = []
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        # Split the text into chunks of 512 tokens
        chunks = [text[i:i+512] for i in range(0, len(text), 512)]
        for chunk in chunks:
            examples.append(tokenizer(chunk, truncation=True, padding='max_length', max_length=512))
        return examples

    examples = encode_book(book_path)
    
    # Create a PyTorch dataset
    dataset = TextDataset(
        tokenizer=tokenizer,
        file_path=book_path,
        block_size=128
    )
    
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, 
        mlm=True,  # Masked Language Model
        mlm_probability=0.15
    )
    
    # Define training arguments
    training_args = TrainingArguments(
        output_dir="../bert_finetuned",
        overwrite_output_dir=True,
        num_train_epochs=3,
        per_device_train_batch_size=4,
        save_steps=10_000,
        save_total_limit=2,
    )
    
    # Initialize Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=dataset,
    )
    
    # Start fine-tuning
    trainer.train()

# Example usage
fine_tune_bert('../datasets/BK900687791.txt')
