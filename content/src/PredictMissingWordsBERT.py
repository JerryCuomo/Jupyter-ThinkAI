# Source: "Think Artificial Intelligence" by Jerry Cuomo, 2024
# Purpose: To demonstrate advanced natural language processing techniques.
# Copyright Â© 2024 Jerry Cuomo. All rights reserved.
#
# This code was autogenerated by GPT-4, based on the following instructions:
# Prompt: Fine-tune BERT for Masked Language Modeling (MLM) on a custom dataset, illustrating the process of pre-processing text, model training, and predictions on masked sentences.
#
# About: This script showcases fine-tuning the BERT model for MLM using a custom text dataset. It involves initializing the BertTokenizer and BertForMaskedLM, pre-processing text for MLM by tokenizing and masking, setting up training with the Trainer and TrainingArguments from the transformers library, and finally, predicting masked words in sentences. The dataset for training is loaded and pre-processed using the datasets library, demonstrating a comprehensive workflow from data preparation to model application.
#
# Setup: Ensure Python is installed along with PyTorch and the transformers and datasets libraries. The custom text dataset 'datasets/BK900687791.txt' should be in your directory. Install the required libraries using 'pip install transformers datasets torch'.

from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments
from datasets import load_dataset
import torch

# Define data file paths
custom_dataset_path = '../datasets/BK900687791.txt'
masked_sentences_file = '../datasets/masked_sentences.txt'
results_directory = '../results'
logs_directory = '../logs'

# Initialize tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Specify the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def preprocess_function(examples):
    # Prepare data for MLM: tokenizing and creating labels for masked tokens
    inputs = tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128, return_tensors="pt")
    inputs['labels'] = inputs.input_ids.detach().clone()

    # Create a mask array
    rand = torch.rand(inputs.input_ids.shape)
    mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102) * (inputs.input_ids != 0)

    # For the selected positions, change the input_id to tokenizer.mask_token_id
    selection = []

    for i in range(inputs.input_ids.shape[0]):
        selection.append(torch.flatten(mask_arr[i].nonzero()).tolist())

    for i in range(inputs.input_ids.shape[0]):
        inputs.input_ids[i, selection[i]] = tokenizer.mask_token_id

    return inputs

# Load and preprocess the dataset
dataset = load_dataset('text', data_files={'train': custom_dataset_path})
tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=["text"])
tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

# Initialize Bert model
model = BertForMaskedLM.from_pretrained('bert-base-uncased').to(device)

# Initalize Training argument 
training_args = TrainingArguments(
    output_dir=results_directory,
    num_train_epochs=1, # number of training session
    per_device_train_batch_size=4,
    logging_dir=logs_directory,
    save_strategy="epoch",  # Save a checkpoint at the end of each epoch
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
)

# Update predict_from_file to use the device correctly
def predict_from_file(file_path, model):
    model.eval()  # Set model to evaluation mode
    with open(file_path, 'r') as file:
        sentences = file.readlines()
        
    for sentence in sentences:
        sentence = sentence.strip()
        inputs = tokenizer(sentence, return_tensors='pt').to(device)  # Move inputs to the correct device
        mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]
        
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = outputs.logits
        
        predicted_index = torch.argmax(predictions[0, mask_token_index.item()], dim=-1).item()
        predicted_token = tokenizer.convert_ids_to_tokens(predicted_index)
        print(f"Original: {sentence}")
        print(f"Predicted: {predicted_token}")

# Fine-tune the model
trainer.train()

# Assuming you save your model after training, reload it
model_path = f'{results_directory}/checkpoint-final'  # Adjust path as needed
model = BertForMaskedLM.from_pretrained(model_path).to(device)  # Reload and allocate to the correct device

# Predicting after fine-tuning
print("\nPredicting after fine-tuning:")
predict_from_file(masked_sentences_file, model)
